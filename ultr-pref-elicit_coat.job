#!/bin/sh
# The folliwing lines instruct Slurm to allocate one GPU.
#SBATCH -o ./logs/%A_%a.out                     
#SBATCH -e ./logs/%A_%a.err 
#SBATCH --array=1-20%20                                                     
#SBATCH --mem=50G
#SBATCH --time=05:00:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1

# Set-up the environment.

# export CUDA_HOME="/usr/local/cuda-10.2"
# export PATH="${CUDA_HOME}/bin:${PATH}"
# export LIBRARY_PATH="${CUDA_HOME}/lib64:${LIBRARY_PATH}"
# export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"


source $PATH_TO_CONDA.sh
conda activate fair_ranking

python src/utils/hyper_param_gen.py 
HPARAMS_FILE=config/hyper_parms.txt
python src/main_kfold.py --data coat -job_id ${SLURM_ARRAY_JOB_ID}_$SLURM_ARRAY_TASK_ID $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)
