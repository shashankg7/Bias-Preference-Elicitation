#!/bin/sh
# The folliwing lines instruct Slurm to allocate one GPU.

#SBATCH -o ./logs/%A_%a.out                     
#SBATCH -e ./logs/%A_%a.err                                                         
#SBATCH --mem=50G
#SBATCH --time=05:00:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1


# export CUDA_HOME="/usr/local/cuda-10.2"
# export PATH="${CUDA_HOME}/bin:${PATH}"
# export LIBRARY_PATH="${CUDA_HOME}/lib64:${LIBRARY_PATH}"
# export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"
source $CONDA_PATH
conda activate fair_ranking

python src/main.py --data synthetic --alpha $1
